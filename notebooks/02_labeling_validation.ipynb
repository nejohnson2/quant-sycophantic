{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Labeling Validation\n",
    "\n",
    "This notebook validates the sycophancy labels from the LLM-as-judge.\n",
    "\n",
    "## Goals\n",
    "1. Inspect label quality and distribution\n",
    "2. Compute inter-rater reliability (if multiple models used)\n",
    "3. Compare LLM labels to lexical heuristics\n",
    "4. Identify potential labeling issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quant_syco.config import LABELS_DIR\n",
    "\n",
    "# Find label files\n",
    "label_files = list(LABELS_DIR.glob('labels_*.parquet'))\n",
    "print(f\"Found {len(label_files)} label file(s):\")\n",
    "for f in label_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged labels\n",
    "merged_files = [f for f in label_files if 'merged' in f.name]\n",
    "if merged_files:\n",
    "    labels = pd.read_parquet(merged_files[0])\n",
    "elif label_files:\n",
    "    labels = pd.read_parquet(label_files[0])\n",
    "else:\n",
    "    raise FileNotFoundError(\"No labels found. Run 'make label' first.\")\n",
    "\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Sycophancy A\n",
    "if 'sycophancy_a' in labels.columns:\n",
    "    labels['sycophancy_a'].value_counts().sort_index().plot(\n",
    "        kind='bar', ax=axes[0, 0], color='steelblue'\n",
    "    )\n",
    "    axes[0, 0].set_title('Sycophancy A Distribution')\n",
    "    axes[0, 0].set_xlabel('Score')\n",
    "\n",
    "# Sycophancy B\n",
    "if 'sycophancy_b' in labels.columns:\n",
    "    labels['sycophancy_b'].value_counts().sort_index().plot(\n",
    "        kind='bar', ax=axes[0, 1], color='steelblue'\n",
    "    )\n",
    "    axes[0, 1].set_title('Sycophancy B Distribution')\n",
    "    axes[0, 1].set_xlabel('Score')\n",
    "\n",
    "# Politeness A\n",
    "if 'politeness_a' in labels.columns:\n",
    "    labels['politeness_a'].value_counts().sort_index().plot(\n",
    "        kind='bar', ax=axes[1, 0], color='forestgreen'\n",
    "    )\n",
    "    axes[1, 0].set_title('Politeness A Distribution')\n",
    "    axes[1, 0].set_xlabel('Score')\n",
    "\n",
    "# Politeness B\n",
    "if 'politeness_b' in labels.columns:\n",
    "    labels['politeness_b'].value_counts().sort_index().plot(\n",
    "        kind='bar', ax=axes[1, 1], color='forestgreen'\n",
    "    )\n",
    "    axes[1, 1].set_title('Politeness B Distribution')\n",
    "    axes[1, 1].set_xlabel('Score')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sycophancy vs Politeness Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sycophancy_a' in labels.columns and 'politeness_a' in labels.columns:\n",
    "    corr = labels[['sycophancy_a', 'politeness_a']].corr().iloc[0, 1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Create joint distribution heatmap\n",
    "    joint = pd.crosstab(labels['sycophancy_a'], labels['politeness_a'], normalize=True)\n",
    "    sns.heatmap(joint, annot=True, fmt='.1%', cmap='YlOrRd', ax=ax)\n",
    "    ax.set_title(f'Sycophancy vs Politeness (r={corr:.2f})')\n",
    "    ax.set_xlabel('Politeness')\n",
    "    ax.set_ylabel('Sycophancy')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(f\"Correlation between sycophancy and politeness: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Labeling Success Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for side in ['a', 'b']:\n",
    "    col = f'label_success_{side}'\n",
    "    if col in labels.columns:\n",
    "        success_rate = labels[col].mean()\n",
    "        print(f\"Side {side.upper()} labeling success rate: {success_rate:.1%}\")\n",
    "        \n",
    "        # Show some failed examples\n",
    "        failed = labels[~labels[col]]\n",
    "        if len(failed) > 0:\n",
    "            print(f\"  Failed samples: {len(failed)}\")\n",
    "            print(f\"  Sample errors: {failed[f'label_error_{side}'].value_counts().head(3).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inter-Rater Reliability (if multiple models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quant_syco.analysis.reliability import compute_irr\n",
    "\n",
    "# Check if we have labels from multiple models\n",
    "all_label_files = list(LABELS_DIR.glob('labels_*_a.parquet'))\n",
    "\n",
    "if len(all_label_files) >= 2:\n",
    "    labels1 = pd.read_parquet(all_label_files[0])\n",
    "    labels2 = pd.read_parquet(all_label_files[1])\n",
    "    \n",
    "    print(f\"Comparing: {all_label_files[0].name} vs {all_label_files[1].name}\\n\")\n",
    "    \n",
    "    irr = compute_irr(labels1, labels2, score_col='sycophancy_a')\n",
    "    \n",
    "    print(\"Sycophancy A Inter-Rater Reliability:\")\n",
    "    print(f\"  Weighted Kappa: {irr['kappa']:.3f} ({irr['interpretation']})\")\n",
    "    print(f\"  95% CI: [{irr['ci_lower']:.3f}, {irr['ci_upper']:.3f}]\")\n",
    "    print(f\"  Exact Agreement: {irr['exact_agreement']:.1%}\")\n",
    "    print(f\"  Within-1 Agreement: {irr['within_1_agreement']:.1%}\")\n",
    "    print(f\"  Spearman r: {irr['spearman_r']:.3f} (p={irr['spearman_p']:.4f})\")\n",
    "else:\n",
    "    print(\"Only one model's labels available.\")\n",
    "    print(\"For IRR, run labeling with a second model:\")\n",
    "    print(\"  make label MODEL=mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare LLM Labels to Lexical Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quant_syco.data.process import build_battle_table\n",
    "from quant_syco.features.lexical import compute_lexical_features\n",
    "from scipy import stats\n",
    "\n",
    "# Load battles and compute lexical features\n",
    "battles = build_battle_table()\n",
    "battles_lex = compute_lexical_features(battles, 'assistant_a')\n",
    "\n",
    "# Merge with labels\n",
    "merged = battles_lex.merge(labels, on='question_id', how='inner')\n",
    "\n",
    "if 'sycophancy_a' in merged.columns and 'lex_sycophancy_total' in merged.columns:\n",
    "    # Correlation\n",
    "    corr, pval = stats.spearmanr(\n",
    "        merged['sycophancy_a'].dropna(), \n",
    "        merged.loc[merged['sycophancy_a'].notna(), 'lex_sycophancy_total']\n",
    "    )\n",
    "    \n",
    "    print(f\"Correlation between LLM sycophancy score and lexical count:\")\n",
    "    print(f\"  Spearman r: {corr:.3f} (p={pval:.4f})\")\n",
    "    \n",
    "    # Mean lexical score by LLM score\n",
    "    print(\"\\nMean lexical sycophancy count by LLM score:\")\n",
    "    grouped = merged.groupby('sycophancy_a')['lex_sycophancy_total'].agg(['mean', 'count'])\n",
    "    print(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manual Inspection of Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples at different sycophancy levels\n",
    "merged_sample = merged.merge(\n",
    "    battles[['question_id', 'user_prompt', 'assistant_a']], \n",
    "    on='question_id'\n",
    ")\n",
    "\n",
    "for level in [0, 1, 2, 3]:\n",
    "    subset = merged_sample[merged_sample['sycophancy_a'] == level]\n",
    "    if len(subset) > 0:\n",
    "        row = subset.sample(1, random_state=level).iloc[0]\n",
    "        print(f\"\\n=== Sycophancy Level {level} ===\")\n",
    "        print(f\"Politeness: {row.get('politeness_a', 'N/A')}\")\n",
    "        print(f\"Reasoning: {row.get('reasoning_a', 'N/A')}\")\n",
    "        print(f\"\\nUser: {str(row['user_prompt'])[:200]}...\")\n",
    "        print(f\"\\nAssistant: {str(row['assistant_a'])[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. If IRR is low (Îº < 0.6), consider:\n",
    "   - Refining the prompt\n",
    "   - Using a more capable model\n",
    "   - Manual annotation of a validation subset\n",
    "\n",
    "2. Continue with `03_descriptive_analysis.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
